"use strict";(globalThis.webpackChunkhumanoid_robot_book=globalThis.webpackChunkhumanoid_robot_book||[]).push([[975],{7541:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"sensors-perception/index","title":"04 - Sensors and Perception","description":"Perception is a critical capability for humanoid robots, allowing them to understand their environment, recognize objects, detect humans, and navigate effectively. This chapter explores the diverse array of sensors used and the techniques for processing their data.","source":"@site/docs/04-sensors-perception/index.mdx","sourceDirName":"04-sensors-perception","slug":"/sensors-perception/","permalink":"/humanoid-robot-book/sensors-perception/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"03 - Kinematics and Dynamics","permalink":"/humanoid-robot-book/kinematics-dynamics/"},"next":{"title":"05 - Actuation and Control","permalink":"/humanoid-robot-book/actuation-control/"}}');var o=i(4848),r=i(8453);const t={},a="04 - Sensors and Perception",c={},l=[{value:"Vision Systems",id:"vision-systems",level:2},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:2},{value:"Tactile Sensors",id:"tactile-sensors",level:2},{value:"Lidar (Light Detection and Ranging)",id:"lidar-light-detection-and-ranging",level:2},{value:"Data Processing and Fusion Techniques",id:"data-processing-and-fusion-techniques",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"04---sensors-and-perception",children:"04 - Sensors and Perception"})}),"\n",(0,o.jsx)(n.p,{children:"Perception is a critical capability for humanoid robots, allowing them to understand their environment, recognize objects, detect humans, and navigate effectively. This chapter explores the diverse array of sensors used and the techniques for processing their data."}),"\n",(0,o.jsx)(n.h2,{id:"vision-systems",children:"Vision Systems"}),"\n",(0,o.jsx)(n.p,{children:"Vision is arguably the most important exteroceptive sense for humanoids, mimicking human sight to provide rich environmental information."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Monocular Cameras:"})," A single camera provides 2D images. Used for object detection, recognition, and tracking (with AI/ML models), and visual odometry (estimating motion from camera frames)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Stereo Cameras:"})," Two cameras placed side-by-side, similar to human eyes, allow for depth perception by triangulating points from two different viewpoints. Essential for 3D mapping and obstacle avoidance."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Depth Cameras (RGB-D):"})," Combine a standard RGB camera with a depth sensor (e.g., infrared projector and sensor, time-of-flight). Popular examples include Intel RealSense and Microsoft Kinect. They directly provide color images along with a depth map for each pixel."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Perception Techniques for Vision:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Recognition and Tracking:"})," Using deep learning models (e.g., CNNs like YOLO, Faster R-CNN) to identify and localize objects in the robot's field of view."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Human Detection and Pose Estimation:"})," Specialized models to detect humans and estimate their joint positions, crucial for human-robot interaction and collaboration."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simultaneous Localization and Mapping (SLAM):"})," Combining visual information with odometry to build a map of an unknown environment while simultaneously localizing the robot within that map."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"ROS Example: Camera Feed Processing (Python)"})}),"\n",(0,o.jsxs)(n.p,{children:["This basic ROS Python node subscribes to a camera topic (e.g., ",(0,o.jsx)(n.code,{children:"/camera/image_raw"}),"), converts the image, and prints its dimensions. In a real application, you would perform more complex image processing here (e.g., OpenCV operations)."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python\n\nimport rospy\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\n\ndef image_callback(msg):\n    rospy.loginfo("Received an image!")\n    try:\n        # Convert your ROS Image message to OpenCV format\n        bridge = CvBridge()\n        cv_image = bridge.imgmsg_to_cv2(msg, desired_encoding="bgr8")\n\n        # Process the image (e.g., print dimensions)\n        (rows, cols, channels) = cv_image.shape\n        rospy.loginfo(f"Image dimensions: {cols}x{rows}")\n\n        # Display the image (optional, requires a display)\n        # cv2.imshow("Camera Feed", cv_image)\n        # cv2.waitKey(1)\n\n    except Exception as e:\n        rospy.logerr(e)\n\ndef camera_subscriber():\n    rospy.init_node(\'camera_subscriber_node\', anonymous=True)\n    rospy.Subscriber("/camera/image_raw", Image, image_callback)\n    rospy.spin()\n\nif __name__ == \'__main__\':\n    try:\n        camera_subscriber()\n    except rospy.ROSInterruptException:\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,o.jsx)(n.p,{children:"Force/torque (F/T) sensors measure the forces and torques applied at specific points, typically wrists, ankles, or the base of the robot. They are essential for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Compliant Control:"})," Allowing the robot to interact softly with the environment or humans, rather than rigidly maintaining a position."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation:"})," Detecting contact, slippage, and applying appropriate grasp forces."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Balance and Walking:"})," Measuring ground reaction forces to maintain stability during bipedal locomotion."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"tactile-sensors",children:"Tactile Sensors"}),"\n",(0,o.jsx)(n.p,{children:"Tactile sensors provide information about physical contact, pressure distribution, and even texture. They are typically arrays of small pressure-sensitive elements embedded in robot fingertips, palms, or other body parts."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Applications:"})," Improving grasping robustness, detecting unexpected contact for safety, and enabling sensitive human-robot physical interaction."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"lidar-light-detection-and-ranging",children:"Lidar (Light Detection and Ranging)"}),"\n",(0,o.jsx)(n.p,{children:"Lidar sensors emit laser pulses and measure the time it takes for the pulses to return, creating highly accurate 2D or 3D point clouds of the environment. They are robust to lighting conditions and provide precise distance measurements."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Applications:"})," Environmental mapping, obstacle detection and avoidance, localization (determining the robot's position within a map), and navigation."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"data-processing-and-fusion-techniques",children:"Data Processing and Fusion Techniques"}),"\n",(0,o.jsxs)(n.p,{children:["Raw sensor data is often noisy, incomplete, or in different formats. ",(0,o.jsx)(n.strong,{children:"Sensor fusion"})," techniques combine data from multiple sensors to create a more robust, accurate, and comprehensive understanding of the robot's state and environment."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Kalman Filters / Extended Kalman Filters (EKF):"})," Widely used for state estimation, combining noisy sensor measurements with a dynamic model of the robot to get a more accurate estimate of position, velocity, and orientation."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Particle Filters:"})," Effective for localization in complex, non-linear environments."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Probabilistic Robotics:"})," Frameworks that deal with uncertainty in sensing and actuation, forming the basis for many modern navigation and perception algorithms."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Transformation Trees (TF in ROS):"})," ROS provides a powerful ",(0,o.jsx)(n.code,{children:"tf"})," (transformations) system to keep track of multiple coordinate frames and transform data between them. This is crucial for combining sensor data from different parts of the robot."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"By effectively integrating and processing data from this diverse range of sensors, humanoid robots can build a rich internal model of their world, enabling intelligent and adaptive behavior."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var s=i(6540);const o={},r=s.createContext(o);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);